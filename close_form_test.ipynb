{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y = ReLU(w * x + b)\n",
    "\n",
    "Active set (where w * x + b > 0) iterative fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Active set size: 5035/16512\n",
      "Iteration 2, Active set size: 16464/16512\n",
      "Iteration 3, Active set size: 16399/16512\n",
      "Iteration 4, Active set size: 16373/16512\n",
      "Iteration 5, Active set size: 16368/16512\n",
      "Converged after 5 iterations.\n",
      "Final MSE on test set: 0.4827624956396167\n",
      "Final MSE on train set: 0.45740270445540165\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load and prepare the California Housing dataset\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Standardize features and reshape target to a column vector\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "# Add a column of ones to X to include the bias term\n",
    "X = np.hstack([X, np.ones((X.shape[0], 1))])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "np.random.seed(42)\n",
    "# Random initialization of weights, scaled down to avoid large initial values\n",
    "w = np.random.randn(X_train.shape[1], 1) * 0.01  # Adjusted for bias term\n",
    "\n",
    "max_iterations = 100\n",
    "previous_S = None\n",
    "\n",
    "for iteration in range(max_iterations):\n",
    "    z = X_train @ w\n",
    "    S = z.flatten() > 0  # Active set where z_i > 0\n",
    "\n",
    "    # Check for convergence\n",
    "    if previous_S is not None and np.array_equal(S, previous_S):\n",
    "        print(f\"Converged after {iteration} iterations.\")\n",
    "        break\n",
    "    previous_S = S.copy()\n",
    "\n",
    "    # Subset matrices and vectors based on active set S\n",
    "    X_S = X_train[S]\n",
    "    y_S = y_train[S]\n",
    "\n",
    "    # Solve for w using the closed-form solution\n",
    "    w = np.linalg.pinv(X_S.T @ X_S) @ X_S.T @ y_S\n",
    "    \n",
    "    print(f\"Iteration {iteration + 1}, Active set size: {np.sum(S)}/{len(X_train)}\")\n",
    "\n",
    "else:\n",
    "    print(\"Maximum iterations reached without convergence.\")\n",
    "\n",
    "# Compute final SSE on the test set\n",
    "z_test = X_test @ w\n",
    "y_pred_test = np.maximum(0, z_test)\n",
    "SSE_test = np.sum((y_test - y_pred_test) ** 2)\n",
    "print(\"Final MSE on test set:\", SSE_test / len(y_test))\n",
    "\n",
    "# Compute final SSE on the training set\n",
    "z_train = X_train @ w\n",
    "y_pred_train = np.maximum(0, z_train)\n",
    "SSE_train = np.sum((y_train - y_pred_train) ** 2)\n",
    "print(\"Final MSE on train set:\", SSE_train / len(y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.79207735],\n",
       "       [ 0.13726904],\n",
       "       [-0.18946688],\n",
       "       [ 0.25255451],\n",
       "       [ 0.04693232],\n",
       "       [-3.79466347],\n",
       "       [-0.99214778],\n",
       "       [-0.90929532],\n",
       "       [ 2.00451055]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.79604198],\n",
       "       [ 0.13599718],\n",
       "       [-0.19746062],\n",
       "       [ 0.2609147 ],\n",
       "       [ 0.04340834],\n",
       "       [-3.54717451],\n",
       "       [-0.98981705],\n",
       "       [-0.91022543]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing to gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/2000, Loss: 8688.868823921486\n",
      "Epoch 100/2000, Loss: 8343.769116423062\n",
      "Epoch 150/2000, Loss: 8222.973305135594\n",
      "Epoch 200/2000, Loss: 8141.520770507027\n",
      "Epoch 250/2000, Loss: 8075.374669593973\n",
      "Epoch 300/2000, Loss: 8020.006917634892\n",
      "Epoch 350/2000, Loss: 7965.582213307688\n",
      "Epoch 400/2000, Loss: 7911.849656344966\n",
      "Epoch 450/2000, Loss: 7863.029718076686\n",
      "Epoch 500/2000, Loss: 7821.206848639167\n",
      "Epoch 550/2000, Loss: 7785.630605651713\n",
      "Epoch 600/2000, Loss: 7754.931054252702\n",
      "Epoch 650/2000, Loss: 7728.262758130516\n",
      "Epoch 700/2000, Loss: 7705.418210581371\n",
      "Epoch 750/2000, Loss: 7685.980247568649\n",
      "Epoch 800/2000, Loss: 7669.231109098092\n",
      "Epoch 850/2000, Loss: 7653.282672283955\n",
      "Epoch 900/2000, Loss: 7639.613421946356\n",
      "Epoch 950/2000, Loss: 7627.872472542103\n",
      "Epoch 1000/2000, Loss: 7617.733406607577\n",
      "Epoch 1050/2000, Loss: 7609.082334531049\n",
      "Epoch 1100/2000, Loss: 7601.693420503311\n",
      "Epoch 1150/2000, Loss: 7595.308404373517\n",
      "Epoch 1200/2000, Loss: 7589.689095780285\n",
      "Epoch 1250/2000, Loss: 7584.91482479327\n",
      "Epoch 1300/2000, Loss: 7580.78899794311\n",
      "Epoch 1350/2000, Loss: 7577.236011526586\n",
      "Epoch 1400/2000, Loss: 7574.134425514607\n",
      "Epoch 1450/2000, Loss: 7571.476467504721\n",
      "Epoch 1500/2000, Loss: 7569.185561976749\n",
      "Epoch 1550/2000, Loss: 7567.193990587309\n",
      "Epoch 1600/2000, Loss: 7565.452430788512\n",
      "Epoch 1650/2000, Loss: 7563.921533939752\n",
      "Epoch 1700/2000, Loss: 7562.535541652984\n",
      "Epoch 1750/2000, Loss: 7561.319232879111\n",
      "Epoch 1800/2000, Loss: 7560.265781684574\n",
      "Epoch 1850/2000, Loss: 7559.32518163181\n",
      "Epoch 1900/2000, Loss: 7558.469315843194\n",
      "Epoch 1950/2000, Loss: 7557.725126186066\n",
      "Epoch 2000/2000, Loss: 7557.083807105492\n",
      "Final MSE on test set: 0.4834127701918984\n",
      "Final MSE on train set: 0.45767150960075315\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load and prepare the California Housing dataset\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Standardize features and reshape target to a column vector\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize weights and bias\n",
    "D = X_train.shape[1]\n",
    "\n",
    "np.random.seed(42)\n",
    "w = np.random.randn(D, 1) * 0.01\n",
    "b = np.random.randn(1) * 0.01  # Initialize bias\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.00001\n",
    "num_epochs = 2000\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    Xw = X_train @ w + b  # Include bias term\n",
    "    y_pred = np.maximum(0, Xw)  # ReLU activation\n",
    "\n",
    "    # Compute loss\n",
    "    loss = np.sum((y_train - y_pred) ** 2)\n",
    "\n",
    "    # Compute gradient\n",
    "    relu_grad = (Xw > 0).astype(float)  # Derivative of ReLU\n",
    "    error = y_pred - y_train\n",
    "    grad_w = 2 * X_train.T @ (relu_grad * error)\n",
    "    grad_b = 2 * np.sum(relu_grad * error)\n",
    "\n",
    "    # Update weights and bias\n",
    "    w -= learning_rate * grad_w\n",
    "    b -= learning_rate * grad_b\n",
    "\n",
    "    # Optionally print loss every 100 epochs\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss}\")\n",
    "\n",
    "# After training, w and b contain the optimized weights and bias\n",
    "# print(\"Optimized weights:\\n\", w)\n",
    "# print(\"Optimized bias:\\n\", b)\n",
    "\n",
    "# Compute SSE on the test set\n",
    "Xw_test = X_test @ w + b\n",
    "y_test_pred = np.maximum(0, Xw_test)\n",
    "SSE_test = np.sum((y_test - y_test_pred) ** 2)\n",
    "print(\"Final MSE on test set:\", SSE_test / len(y_test))\n",
    "\n",
    "# Compute SSE on the training set\n",
    "Xw_train = X_train @ w + b\n",
    "y_pred_train = np.maximum(0, Xw_train)\n",
    "SSE_train = np.sum((y_train - y_pred_train) ** 2)\n",
    "print(\"Final MSE on train set:\", SSE_train / len(y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
